{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook makes sure that the rewritten GLM preprocessing pipelines works the same as before.\n",
    "\n",
    "they are compared against files `glm_fitting_preprocessed_all.hdf5` and `glm_fitting_preprocessed_OT.hdf5` under `/private_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tang_jcompneuro import glm_data_preprocessing\n",
    "import h5py\n",
    "import os.path\n",
    "import numpy as np\n",
    "from tang_jcompneuro import dir_dictionary\n",
    "from tang_jcompneuro.io import load_image_dataset, neural_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_show(name, obj):\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(name, obj.shape)\n",
    "\n",
    "def list_datasets():\n",
    "    with h5py.File(os.path.join(dir_dictionary['private_data'], 'glm_fitting_preprocessed_all.hdf5'), 'r') as f:\n",
    "        f.visititems(callback_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MkA_Shape/half/0/None (9500, 800)\n",
      "MkA_Shape/half/2/pca_random_sep (9500, 1032)\n",
      "MkA_Shape/half/4/pca_random_sep (9500, 1032)\n",
      "MkA_Shape/half/8/pca_random_sep (9500, 1032)\n",
      "MkA_Shape/half/fpower/None (9500, 400)\n",
      "MkA_Shape/half/linear/None (9500, 400)\n",
      "MkA_Shape/y (9500, 1142)\n",
      "MkE2_Shape/half/0/None (4605, 800)\n",
      "MkE2_Shape/half/2/pca_random_sep (4605, 1032)\n",
      "MkE2_Shape/half/4/pca_random_sep (4605, 1032)\n",
      "MkE2_Shape/half/8/pca_random_sep (4605, 1032)\n",
      "MkE2_Shape/half/fpower/None (4605, 400)\n",
      "MkE2_Shape/half/linear/None (4605, 400)\n",
      "MkE2_Shape/y (4605, 979)\n"
     ]
    }
   ],
   "source": [
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_mapping = {\n",
    "    'linear': 'half/linear/None',\n",
    "    'fpower': 'half/fpower/None',\n",
    "    'gqm.2': 'half/2/pca_random_sep',\n",
    "    'gqm.4': 'half/4/pca_random_sep',\n",
    "    'gqm.8': 'half/8/pca_random_sep',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_dict = glm_data_preprocessing.generate_transformer_dict(glm_data_preprocessing.max_total_dim_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fpower': (tang_jcompneuro.glm_data_preprocessing.FPGLMPreprocessor, {}),\n",
       " 'gqm.2': (tang_jcompneuro.glm_data_preprocessing.GQMPreprocessor,\n",
       "  {'locality': 2, 'max_total_dim': 1032}),\n",
       " 'gqm.4': (tang_jcompneuro.glm_data_preprocessing.GQMPreprocessor,\n",
       "  {'locality': 4, 'max_total_dim': 1032}),\n",
       " 'gqm.8': (tang_jcompneuro.glm_data_preprocessing.GQMPreprocessor,\n",
       "  {'locality': 8, 'max_total_dim': 1032}),\n",
       " 'linear': (tang_jcompneuro.glm_data_preprocessing.VanillaGLMPreprocessor, {})}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del transformer_dict['gqm.4']\n",
    "# del transformer_dict['gqm.8']\n",
    "# del transformer_dict['linear']\n",
    "# del transformer_dict['fpower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from strflab.feature_transformation import quadratic_features\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def legacy_trans(X, loc, new_trans=False):\n",
    "    x_flat = X.reshape(len(X), -1)\n",
    "    X_flat_q = quadratic_features(X, locality=(0, loc, loc))\n",
    "    # let's check amount of variance contributed by X_flat and X_flat_q\n",
    "    x_flat_all_this = np.concatenate((x_flat, X_flat_q), axis=1)\n",
    "#     print(x_flat_all_this.shape, 'debug legacy')\n",
    "    assert x_flat.shape[1] == 400\n",
    "    size_x_linear = 400\n",
    "    \n",
    "    threshold_num_feature = 1032\n",
    "    num_feature = x_flat_all_this.shape[1]\n",
    "\n",
    "#     if num_feature <= threshold_num_feature:\n",
    "#         # sometimes, X_flat_all_this.shape[0] can be small, so that in the end PCA have even\n",
    "#         # smaller dims than original. That's fine. We don't do cross-dataset testing,\n",
    "#         # and won't have any issue.\n",
    "#         method_name = 'None'\n",
    "#     else:\n",
    "#         # do PCA.\n",
    "#         method_name = 'pca_random_sep'\n",
    "    \n",
    "#     assert method_name == 'pca_random_sep'\n",
    "    \n",
    "#     if method_name == 'None':\n",
    "#         explained_var = 1.0\n",
    "#         pca_obj = PCA(svd_solver='full', n_components=None)\n",
    "#         x_to_use = pca_obj.fit_transform(x_flat_all_this)\n",
    "#         # remixing makes convergence faster.\n",
    "#     elif method_name == 'pca_random_sep':\n",
    "    pca_feature = min(num_feature - size_x_linear,\n",
    "                      threshold_num_feature - size_x_linear,\n",
    "                      x_flat_all_this.shape[0])\n",
    "    assert pca_feature > 1\n",
    "    pca_obj = PCA(svd_solver='randomized', n_components=pca_feature,\n",
    "                  random_state=0)\n",
    "    pca_sep_input = x_flat_all_this[:, size_x_linear:]\n",
    "    \n",
    "#     print(pca_sep_input.std(), 'legacy debug')\n",
    "    \n",
    "    if new_trans:\n",
    "        # this is used in new transformer code.\n",
    "        x_to_use = pca_obj.fit(pca_sep_input).transform(pca_sep_input)\n",
    "    else:\n",
    "        # this is used in original code.\n",
    "        x_to_use = pca_obj.fit_transform(pca_sep_input)\n",
    "#     print(x_to_use.std(), x_to_use.mean(), x_to_use2.std(), x_to_use2.mean(), 'legacy debug')\n",
    "    \n",
    "#     print(pearsonr(x_to_use.ravel(), x_to_use2.ravel())[0],\n",
    "#          abs(x_to_use2-x_to_use).max())\n",
    "    \n",
    "    assert x_to_use.shape[1] == pca_feature\n",
    "    explained_var = np.cumsum(pca_obj.explained_variance_ratio_)\n",
    "    pca_sep_final_input = np.concatenate([x_flat_all_this[:, :size_x_linear],\n",
    "                                          x_to_use], axis=1)\n",
    "    \n",
    "#     print(pca_sep_final_input1.std(), pca_sep_final_input1.mean(),\n",
    "#           pca_sep_final_input2.std(), pca_sep_final_input2.mean(), 'legacy debug final output')\n",
    "#     print(abs(pca_sep_final_input1-pca_sep_final_input2).max(), 'legacy debug final output')\n",
    "    pca_obj = PCA(svd_solver='full', n_components=None)\n",
    "    x_to_use = pca_obj.fit_transform(pca_sep_final_input)\n",
    "    return x_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_one_case(neural_dataset, subset):\n",
    "    # load X in this neural dataset, using legacy method of resizing image.\n",
    "    image_key = neural_dataset_dict[neural_dataset]['image_dataset_key']\n",
    "    imageX = load_image_dataset(image_key, trans=True, scale=0.5, subset=subset, legacy_rescale=True)\n",
    "    imageX_copy = imageX.copy()\n",
    "    # then generate transformers\n",
    "    #\n",
    "    rng_state = np.random.RandomState(seed=0)\n",
    "    \n",
    "    for k, (class_this, kwargs) in transformer_dict.items():\n",
    "        print(k)\n",
    "        transformer_this: glm_data_preprocessing.GLMDataPreprocesser = class_this(**kwargs)\n",
    "        transformer_this.get_transformer(imageX)\n",
    "        explained_var_ratio_cumsum = np.cumsum(transformer_this.per_dim_var)\n",
    "        X_transed = transformer_this.transform(imageX)\n",
    "        # print(X_transed.shape)\n",
    "        # assert explained_var_ratio_cumsum.shape == (X_transed.shape[1],)\n",
    "        \n",
    "        # compare with reference data.\n",
    "        with h5py.File(os.path.join(dir_dictionary['private_data'], f'glm_fitting_preprocessed_{subset}.hdf5'), 'r') as f_ref:\n",
    "            X_transed_ref = f_ref[f'{neural_dataset}/' + key_mapping[k]][...]\n",
    "            explained_var_ratio_cumsum_ref = f_ref[f'{neural_dataset}/' + key_mapping[k]].attrs['explained_var']\n",
    "        \n",
    "        if k in {'linear', 'fpower'}:\n",
    "            assert explained_var_ratio_cumsum_ref == 1.0\n",
    "            assert abs(explained_var_ratio_cumsum[-1]-1) < 1e-6\n",
    "        else:\n",
    "            assert explained_var_ratio_cumsum_ref.shape == explained_var_ratio_cumsum.shape\n",
    "            assert abs(explained_var_ratio_cumsum-explained_var_ratio_cumsum_ref).max() < 1e-6\n",
    "        \n",
    "        \n",
    "        assert X_transed.shape == X_transed_ref.shape\n",
    "        print(abs(X_transed_ref-X_transed).max())\n",
    "        \n",
    "        \n",
    "        if not abs(X_transed_ref-X_transed).max() < 1e-6:\n",
    "            assert 'gqm' in k\n",
    "            print('NOT match! but possibly due to difference of fit.transform vs fit_transform')\n",
    "            print(np.sum(abs(X_transed_ref-X_transed)>=1e-6))\n",
    "            plt.close('all')\n",
    "            plt.hist(X_transed_ref.ravel()-X_transed.ravel())\n",
    "            plt.show()\n",
    "            \n",
    "            X_transed_legacy_oldtrans = legacy_trans(imageX, transformer_this.locality, False)\n",
    "            assert X_transed.shape == X_transed_legacy_oldtrans.shape\n",
    "            print('debug', abs(X_transed_legacy_oldtrans-X_transed_ref).max())\n",
    "            assert abs(X_transed_legacy_oldtrans-X_transed_ref).max() < 1e-6\n",
    "            \n",
    "            X_transed_legacy_newtrans = legacy_trans(imageX, transformer_this.locality, True)\n",
    "            assert X_transed.shape == X_transed_legacy_newtrans.shape\n",
    "        \n",
    "            print('debug', abs(X_transed_legacy_newtrans-X_transed).max())\n",
    "            assert abs(X_transed_legacy_newtrans-X_transed).max() < 1e-6\n",
    "        \n",
    "        \n",
    "        \n",
    "#         assert abs(X_transed_ref-X_transed).max() < 1e-6\n",
    "        # finally, (partially) make sure that I did not mistakenly use old data.\n",
    "        perm_idx_this = rng_state.permutation(imageX.shape[0])\n",
    "        \n",
    "        X_transed_debug = np.empty_like(X_transed)\n",
    "        \n",
    "#         for idx_aaa, idx_this in enumerate(perm_idx_this):\n",
    "#             if idx_aaa % 10 == 0:\n",
    "#                 print(idx_aaa)\n",
    "#             X_transed_debug[idx_this:idx_this+1] = transformer_this.transform(imageX[idx_this:idx_this+1])\n",
    "        # break to two blocks\n",
    "        X_transed_debug[perm_idx_this[:100]] =  transformer_this.transform(imageX[perm_idx_this[:100]])\n",
    "        X_transed_debug[perm_idx_this[100:]] =  transformer_this.transform(imageX[perm_idx_this[100:]])\n",
    "        \n",
    "        assert X_transed.shape == X_transed_debug.shape\n",
    "        print(abs(X_transed_debug-X_transed).max(), 'difference between one-batch and two-batch with permutation')\n",
    "        # this is not zero probably due to blas lib.\n",
    "        assert abs(X_transed_debug-X_transed).max() < 1e-6\n",
    "        \n",
    "        # finally, check statelessness.\n",
    "        assert np.array_equal(transformer_this.transform(imageX_copy),\n",
    "                              X_transed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "9.85878045867e-14\n",
      "7.1054273576e-15 difference between one-batch and two-batch with permutation\n",
      "fpower\n",
      "2.77555756156e-15\n",
      "6.66133814775e-16 difference between one-batch and two-batch with permutation\n",
      "gqm.2\n",
      "3.22279009035e-12\n",
      "1.7763568394e-14 difference between one-batch and two-batch with permutation\n",
      "gqm.4\n",
      "1.14841469667e-11\n",
      "4.26325641456e-14 difference between one-batch and two-batch with permutation\n",
      "gqm.8\n",
      "1.83462134373e-11\n",
      "4.26325641456e-14 difference between one-batch and two-batch with permutation\n"
     ]
    }
   ],
   "source": [
    "check_one_case('MkA_Shape', 'OT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "2.02948768901e-13\n",
      "3.5527136788e-15 difference between one-batch and two-batch with permutation\n",
      "fpower\n",
      "6.69603261727e-15\n",
      "2.77555756156e-16 difference between one-batch and two-batch with permutation\n",
      "gqm.2\n",
      "2.65681496192\n",
      "NOT match! but possibly due to difference of fit.transform vs fit_transform\n",
      "9714840\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa985ad6518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug 0.0\n",
      "debug 7.38964445191e-13\n",
      "4.97379915032e-14 difference between one-batch and two-batch with permutation\n",
      "gqm.4\n",
      "7.61675028604e-11\n",
      "3.5527136788e-14 difference between one-batch and two-batch with permutation\n",
      "gqm.8\n",
      "2.11581863141e-11\n",
      "5.68434188608e-14 difference between one-batch and two-batch with permutation\n"
     ]
    }
   ],
   "source": [
    "check_one_case('MkA_Shape', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "9.28146448587e-14\n",
      "3.5527136788e-15 difference between one-batch and two-batch with permutation\n",
      "fpower\n",
      "1.49880108324e-15\n",
      "5.55111512313e-16 difference between one-batch and two-batch with permutation\n",
      "gqm.2\n",
      "2.04103400847e-12\n",
      "2.48689957516e-14 difference between one-batch and two-batch with permutation\n",
      "gqm.4\n",
      "9.85991843727e-12\n",
      "2.48689957516e-14 difference between one-batch and two-batch with permutation\n",
      "gqm.8\n",
      "1.01152419774e-11\n",
      "5.68434188608e-14 difference between one-batch and two-batch with permutation\n"
     ]
    }
   ],
   "source": [
    "check_one_case('MkE2_Shape', 'OT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "8.59937121511e-14\n",
      "3.99680288865e-15 difference between one-batch and two-batch with permutation\n",
      "fpower\n",
      "2.12753457399e-15\n",
      "2.22044604925e-16 difference between one-batch and two-batch with permutation\n",
      "gqm.2\n",
      "1.13438147764e-11\n",
      "4.26325641456e-14 difference between one-batch and two-batch with permutation\n",
      "gqm.4\n",
      "2.46922482461e-11\n",
      "3.5527136788e-14 difference between one-batch and two-batch with permutation\n",
      "gqm.8\n",
      "2.88049584185e-11\n",
      "3.5527136788e-14 difference between one-batch and two-batch with permutation\n"
     ]
    }
   ],
   "source": [
    "check_one_case('MkE2_Shape', 'all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
